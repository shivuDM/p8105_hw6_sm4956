p8105_hw6_sm4956
================
Shivangi Deepak Mewada

## Problem 0

-   created repository and R project for HW6, created rmd file and
    rending to GitHub.
-   created a sub-directory/ data folder for the data set files to be
    used for this HW

``` r
library(tidyverse)
library(readxl)
library(dplyr)
library(ggridges)
library(tibble)
library(broom)
library(purrr)
library(modelr)
library(mgcv)
options(tibble.print_min = 5)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 1

Obtaining the dataset for 2017 Central Park Weather data and cleaning
it.

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

To obtain a distribution for $\hat{r}^2$: drawing bootstrap samples,
extracting the values and summarizing. Using `modelr::bootstrap` to draw
the samples and `broom::glance` to produce `r.squared` values. Creating
plot for estimate distribution.

``` r
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

<img src="p8105_hw6_sm4956_files/figure-gfm/unnamed-chunk-1-1.png" width="90%" />

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1
may be a cause for the generally skewed shape of the distribution. If we
wanted to construct a confidence interval for $R^2$, we could take the
2.5% and 97.5% quantiles of the estimates across bootstrap samples.
However, because the shape isn’t symmetric, using the mean +/- 1.96
times the standard error probably wouldn’t work well.

a distribution for $\log(\beta_0 * \beta1)$ can be done using a similar
approach, with a bit more wrangling before making the plot.

``` r
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

<img src="p8105_hw6_sm4956_files/figure-gfm/unnamed-chunk-2-1.png" width="90%" />

## Problem 2

creating dataset for homicides. Data wrangling for: creating a new
variable with city_state, creating binary variable for solved based on
disposition, removing un-required states, keeping only White and Black
races, converting victim_age to numeric form.

``` r
homicide_ds = read_csv(
    "data/homicide-data.csv") %>%
  janitor::clean_names() %>%
  unite('city_state',"city":"state", remove = FALSE)%>%
  select(-city,-state)%>%
  mutate (solved = case_when (disposition == "Closed without arrest" ~ "Unresolved", 
                              disposition == "Open/No arrest" ~ "Unresolved", 
                              disposition == "Closed by arrest" ~ "Resolved" ),
          solved = as.factor (solved),
          solved = fct_relevel(solved, "Unresolved"),
          victim_age = as.numeric(victim_age)) %>%
  filter (!(city_state %in% c("Tulsa_AL","Dallas_TX", "Phoenix_AZ", "Kansas City_MO"))) %>%
  filter (!(victim_race %in% c("Asian", "Hispanic","Other", "Unknown"))) 
```
