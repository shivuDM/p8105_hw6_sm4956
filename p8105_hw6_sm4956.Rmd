---
title: "p8105_hw6_sm4956"
author: Shivangi Deepak Mewada
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE)
```

## Problem 0
* created repository and R project for HW6, created rmd file and rending to GitHub. 
* created a sub-directory/ data folder for the data set files to be used for this HW

```{r load libraries}
library(tidyverse)
library(readxl)
library(dplyr)
library(ggridges)
library(tibble)
library(broom)
library(purrr)
library(modelr)
library(mgcv)
options(tibble.print_min = 5)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Obtaining the dataset for 2017 Central Park Weather data and cleaning it.
```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

To obtain a distribution for $\hat{r}^2$: drawing bootstrap samples, extracting the values and summarizing. 
Using `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. Creating plot for estimate distribution. 
```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

a distribution for $\log(\beta_0 * \beta1)$ can be done using a similar approach, with a bit more wrangling before making the plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

## Problem 2

creating dataset for homicides. Data wrangling for: creating a new variable with city_state, creating binary variable for solved based on disposition, removing un-required states, keeping only White and Black races, converting victim_age to numeric form. 
```{r summarizing for homicides}
homicide_ds = read_csv(
    "data/homicide-data.csv") %>%
  janitor::clean_names() %>%
  unite('city_state',"city":"state", remove = FALSE)%>%
  select(-city,-state)%>%
  mutate (solved = case_when (disposition == "Closed without arrest" ~ "Unresolved", 
                              disposition == "Open/No arrest" ~ "Unresolved", 
                              disposition == "Closed by arrest" ~ "Resolved" ),
          solved = as.factor (solved),
          solved = fct_relevel(solved, "Unresolved"),
          victim_age = as.numeric(victim_age)) %>%
  filter (!(city_state %in% c("Tulsa_AL","Dallas_TX", "Phoenix_AZ", "Kansas City_MO"))) %>%
  filter (!(victim_race %in% c("Asian", "Hispanic","Other", "Unknown"))) 
```

Filtering for Baltimore city, using the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. Applying broom::tidy to obtain estimate and confidence intervals for adjusted OR. Data wrangling for adj OR and 95% CI for the OR. 
```{r}
baltimore_homi = homicide_ds %>%
  filter(city_state == "Baltimore_MD")

  baltimore_log = baltimore_homi %>%
  glm(solved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) %>%
  broom::tidy() %>%
  mutate (adj_OR = exp(estimate),
          lower_95ci = exp(estimate - (1.96 * std.error)),
         upper_95ci = exp(estimate + (1.96 * std.error))) %>%
    filter (term == "victim_sexMale") %>%
    select (adj_OR, lower_95ci, upper_95ci)
  
    knitr::kable((baltimore_log), digits = 3)
```

* In Baltimore, the odds of solving homicides comparing male victims to female victims are **0.426 (95% CI: 0.325,0.558)**, adjusting for age and race.

Applying glm for all the cities, and extracting the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. Using purrr::map and unnest to create a dataframe 'cities_log' with estimated ORs and 95% CIs for each city. 
```{r OR and 95% CI for all cities}
cities_log = homicide_ds %>%
  group_by (city_state) %>%
  nest () %>%
  mutate(model_cities = map(.x = data, ~ glm(solved ~ victim_age + victim_race + victim_sex, data = ., family = binomial())),
         model_cities = map(model_cities, broom::tidy)) %>%
  unnest(model_cities) %>%
  mutate (adj_OR = exp(estimate),
          lower_95ci = exp(estimate - (1.96 * std.error)),
         upper_95ci = exp(estimate + (1.96 * std.error))) %>%
  filter (term == "victim_sexMale") %>%
  select (city_state, adj_OR, lower_95ci, upper_95ci) %>%
  ungroup()
  
```

Plotting the ORs and 95% CI for all cities, arranging in order according to the estimated ORs.
```{r cities model plot}
plot_cities_model = cities_log %>%
  mutate (city_state = reorder (city_state, adj_OR)) %>%
  ggplot(aes (y = city_state, x = adj_OR)) + 
  geom_point() + 
  geom_errorbar(aes(xmin = lower_95ci, xmax = upper_95ci)) + 
  theme(legend.position = "bottom", axis.text.x = element_text( hjust = 1)) + 
  labs (title = "Plot for OR estimates across cities and states",
        x = "ORs with 95% CI",
        y = "City and State")
  
plot_cities_model
```

By eyeballing the plot, it can be seen that **most of the cities have OR less than 1**. Cities like **Atlanta, Richmond, Nashville, Fresno, Stockton and Albuquerque have OR >= 1**. It means that the odds of solving homicide among male victims in most of the cities except for the ones stated above are less than those of solving homicides among female victims, adjusting for age and race. 
The **95% confidence intervals for about half of the cities seem to include 1**, suggesting they are **not statistically significant**.

## Problem 3

Obtaining dataset for birth weight of the babies from the sub-directory. Data wrangling to convert binary variables to factor form, re-coding variables as per data dictionary provided, and changing SI units to maintain uniformity across the variables. 
```{r}
birthweight_ds = read_csv(
    "data/birthweight.csv") %>%
  janitor::clean_names() %>%
  mutate (babysex = as.factor(babysex),
          frace = as.factor(frace),
          mrace = as.factor(mrace),
          malform = as.factor(malform)) %>%
  mutate (babysex = fct_recode(babysex, "male" = "1", "female" = "2"),
          frace = fct_recode(frace, "White" = "1", "Black" = "2", "Asian" = "3", "Puerto Rican" = "4", "Other" = "8",     "Unknown" = "9"),
          mrace = fct_recode(mrace, "White" = "1", "Black" = "2", "Asian" = "3", "Puerto Rican" = "4", "Other" = "8"),
          malform = fct_recode(malform, "absent" = "0", "present" = "1")) %>%
  mutate (delwt_gm = delwt * 453.592,
          ppwt_gm = ppwt * 453.592,
          wtgain_gm = wtgain * 453.592,
          mheight_cm = mheight * 2.54) %>%
  select (-delwt, -ppwt, -wtgain, -mheight)
```

- The total number of observations/rows are **`r nrow(birthweight_ds)`** and the total number of variables/columns are **`r ncol(birthweight_ds)`**. The key variables in this data set are **`r colnames(birthweight_ds)`**.

To develop a regression model for birthweight, I am proposing a model based on hypothesized structure of factors underlying birthweight of the baby. I discussed these factors with a friend who is practicing Gynaecology/Obstetrics, and finalized the variables accordingly. I am including the following variables in my model:
- **babysex** : birthweight is usually determined to be higher among males than females
- **delwt_gm** : overweight mothers can have babies with birthweight higher than the average birthweight
- **momage** : higher the maternal age, lower can be the birthweight
- **wtgain_gm** : higher the weight mother gained during pregnancy, higher can be the birthweight
- **smoken** : birthweight will be lower if mother is a smoker

```{r proposing regression model with hypothesizing factors underlying birth weight}
bw_model1 = lm (bwt ~ babysex + delwt_gm + momage + wtgain_gm + smoken, data = birthweight_ds)
```

Plotting model residuals against fitted values:
```{r plot of model residuals against fitted values}
birthweight_ds %>%
  add_residuals (bw_model1) %>%
  add_predictions (bw_model1) %>%
  ggplot(aes (x = pred, y = resid)) + 
  geom_point () + geom_smooth(se = FALSE) +
  labs (title = "Plot of model residuals against fitted values",
        x = "fitted values",
        y = "residuals")
```

**Comparing** the model with 2 other models:
Main model: uses length at birth and gestational age as predictors 
Interaction model: uses head circumference, length, sex, and all interactions (including the three-way interaction) between these
```{r other models}
main_model = lm (bwt ~ blength + gaweeks, data = birthweight_ds)
interact_model = lm (bwt ~ bhead + blength + babysex + (bhead*blength) + (bhead*babysex) + (blength*babysex) + (bhead*blength*babysex), data = birthweight_ds)

```

comparing the 3 models in terms of the cross-validated prediction error; using crossv_mc. Showing comparison using plots.
```{r comparing all models}
cv_bw_ds = crossv_mc(birthweight_ds, 100)

cv_bw_ds = cv_bw_ds %>%
  mutate (
    bw_model1 = map(train, ~lm(bwt ~ babysex + delwt_gm + momage + wtgain_gm + smoken, data = .x)),
    main_model = map(train, ~lm (bwt ~ blength + gaweeks, data = .x)),
    interact_model = map (train, ~lm (bwt ~ bhead + blength + babysex + (bhead*blength) + (bhead*babysex) + (blength*babysex) + (bhead*blength*babysex), data = .x))) %>%
  mutate (
    rmse_bw_model1 = map2_dbl(bw_model1, test, ~rmse(model = .x, data = .y)),
    rmse_main_model = map2_dbl(main_model, test, ~rmse(model = .x, data = .y)),
    rmse_interact_model = map2_dbl(interact_model, test, ~rmse(model = .x, data = .y)))
```

```{r plotting to compare}
cv_bw_ds %>%
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_") %>%
  mutate (model = fct_inorder(model)) %>%
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() + 
  labs (title = "Plot comparing birthweight models by cross-validation",
        x = "Model",
        y = "RMSE")
```

The comparison between 3 models suggests that the **3rd model, i.e., the interaction model** is the most appropriate amongst the 3 models. Since it has the **lowest RMSE, it indicates a better fit for the birthweight model**. 